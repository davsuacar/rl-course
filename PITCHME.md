
@snap[north-west] 
RL Course
@snapend
---

@snap[north-west] 
MDP
@snapend

@snap[south-west list-content-concise span-100]
@ol[list-bullets-black](false)

	- S is a finite set of states
	- A is a finite set of actions (alternatively, `\(A_s \)` is the finite set of actions available from state `\(s)\)`
	- `\(T(s'|s, a) = T(s_{t+1} = s' | s_t = s, a_t = a) \)` is the probability that action a in state s at time t will lead to state s' at time t+1
	- `\(R(s, a) \)` is the immediate reward (or expected immediate reward) received after transitioning from state `\(s\)` to state `\(s'\)`, due to action `\(a\)`
	- `\(\gamma âˆˆ [0,1] \)` is the discount factor, which represents the difference in importance between future rewards and present rewards

@olend
<br><br>
@snapend
---

@snap[north-west] 
State Value Function
@snapend
---

@snap[north-west] 
Action Value Function
@snapend
---

@snap[north-west] 
Bellman Equation
@snapend
---

@snap[north-west] 
Model free
@snapend
---

@snap[north-west] 
Model Based
@snapend
---

@snap[north-west] 
On-Policy
@snapend
---

@snap[north-west] 
Off-Policy
@snapend
---

@snap[north-west] 
Policy Iteration
@snapend
---

@snap[north-west] 
Value Iteration
@snapend
---

@snap[north-west] Q-Learning
@snapend
---

@snap[north-west] 
SARSA
@snapend
---
